# 논문 리뷰 템플릿 - 개조식 작성 가이드

> 이 템플릿은 개조식으로 논문을 효과적으로 리뷰하기 위한 구조를 제공하며, 필요에 따라 구조와 내용을 삭제, 수정 및 추가하여 사용


## 1. 논문 기본 정보

- **제목**: AN IMAGE IS WORTH 16X16 WORDS: TRANSFORMERS FOR IMAGE RECOGNITION AT SCALE
- **저자**: Alexey Dosovitskiy, Lucas Beyer, Alexander Kolesnikov, Dirk Weissenborn, Xiaohua Zhai, Thomas Unterthiner, Mostafa Dehghani, Matthias Minderer, Georg Heigold, Sylvain Gelly, Jakob Uszkoreit, Neil Houlsby
- **학회/저널**: International Conference on Learning Representations(ICLR)
- **년도**: 2021
- **DOI/URL**: https://arxiv.org/pdf/2010.11929
- **키워드**: Computer Vision, Vision Transformer(ViT), Inductive bias, Multi-Head Attention, Patch Embedding

## 2. 논문 요약

### 2.1 연구 목적 및 문제 정의
- CNN을 사용하지 않는 새로운 이미지 인식 모델 제안

- 기존 CNN 기반 모델의 문제점:
  ○ 현대 가속기의 효율적 연산 적용 어려움
  ○ Inductive bias로 인한 대량 데이터에서 성능 한계

- 트랜스포머 구조(Multi-Head attention 기반 인코더 활용)를 활용한 이미지 인식 모델 제안

### 2.2 주요 접근 방법
- 트랜스포머 구조를 활용한 이미지 인식 아키텍쳐 제안

- 주요 구성 요소
  ○ Patch+Position Embedding
  ○ 트랜스포머 인코더
  ○ Multi-Layer Perceptron(MLP: 선형변환과 비선형함수로 이루어진 다층 구조)

### 2.3 주요 결과
- ImageNet Top1 accuracy 점수 87.76% 달성(ViT-L/16)
  ○ BiT-L 모델보다 0.22% 향상

- VTAB에서 점수 77.63% 달성(ViT-H/14)
  ○ BiT-L 모델보다 1.34% 향상

- 주요 발견점
  ○ Inductive bias 극복
  ○ Scaling으로 지속적 성능 향상 가능
  ○ 1D Positional Embedding과 2D Positional Embedding 사이 유의미한 차이 없음

## 3. 방법론 분석

### 3.1 제안 방법 상세 설명
- Vision Transformer 구성
  ○ Embedding
    ■ Patch Embedding: Patch를 N개의 1D sequence로 자른 이후, Linear projection을 통해 D dimension 변환
    ■ Position Embedding: NxD Embedding matrix E_pos를 Patch embedding 값에 더해줌
  ○ 인코더
    ■ 각 서브층에 Layer Normalization과 Residual Connection 활용
    ■ 하나의 Multi-Head Attention layer와 Multi-Layer Perceptron(MLP)로 구성
  ○ MLP Head
    ■ 추가적인 class token의 인코더 output을 MLP Head의 input으로 삽입
    ■ Pre-training 시에는 MLP(tanh 활성화 함수)를 사용하나, Fine-tuning 시에는 단일 Linear Layer를 사용
    ■ K dimension으로 변환하여 최종 class 예측

### 3.2 핵심 알고리즘/모델
- Embedding representation:
  ○ 수식: z_0 = [x_class; (x_p)^1*E; (x_p)^2*E;...; (x_p)^N*E]+E_pos
    ■ z_0: Transformer 인코더에 들어가는 초기 input sequence
    ■ x_class: 학습 가능한 [class] token
    ■ x_p^i: i번째 image patch
    ■ E: D dimension 변환 행렬(Linear Projection).
    ■ E_pos: 위치 정보를 담고 있는 embedding vector
  ○ 2D input patch를 1D로 펼쳐서 넣음
  ○ position embedding vector를 추가하여 order에 대한 정보 학습
  ○ 모델이 학습 가능한 D dimension으로 변환

- Multi-Head Attention + Residual Connection:
  ○ 수식: z'_l = MSA(LN(z_(l−1))) + z_(l−1)
    ■ z'_l: l번째 Multi-Head Attention sub-layer의 output 
    ■ z_(l-1): l-1번째 layer의 output
    ■ LN: Layer Normalization
    ■ MSA: Multi-Head Attention 연산(Attention is All you need 리뷰)
  ○ 다양한 관점에서 정보 종합
  ○ 기울기 소실 문제 완화(deep layer 형성 가능)

- Multi-Layer Perceptron(MLP):
  ○ 수식: z_l = MLP(LN(z'_l)) + z'_l
    ■ z_l: l번째 layer의 output
    ■ MLP: 두 개의 linear projection과 하나의 gelu activation function으로 구성

### 3.3 실험 설계
- 데이터셋(pre-training):
  ○ ImageNet (1K classes, 1.3M images)
  ○ ImageNet-21k (21k classes, 14M images)
  ○ JFT (18k classes, 303M images)

- 모델 구성:
  ○ layer 수: 12
  ○ hidden size D dimension(MSA 내부): 768
  ○ MLP dimension: 3072
  ○ head 수: 12
  ○ Parameter 수: 86M

- 최적화 방법:
  ○ Adam optimizer 사용
  ○ Warmup 후 cosine decay 사용
  ○ Dropout 사용
  ○ Label smoothing 사용

## 4. 주요 결과 분석

### 4.1 정량적 결과
- 이미지 인식
  ○ ImageNet: 87.76%(ViT-L/16)(BiT-L 대비 +0.022%)
  ○ VTAB(19 tasks): 77.63%(ViT-H/14)(BiT-L 대비 +1.34%)

- 계산 효율성:
  ○ 계산 크기가 동일하다면, ViT가 BiT보다 정확

- 통계적 유의성
  ○ 만일 Pre-training dataset이 작다면, BiT가 더 좋은 성능을 가질 수 있음  

### 4.2 정성적 결과
- CV 분야에서 attention 기법을 활용하여 medium-level의 resolution을 다룸
- Inductive bias 극복(saturation되지 않음, 초기 layer에서도 global information을 가짐)
- Self-supervised ViT의 가능성 보임
- 1D와 2D positional embedding간 차이가 크지 않음(계산 효율성을 위해 1D 사용)
- Attention rollout으로 semantically relevant regions을 정확하게 학습함을 증명

### 4.3 비교 분석
- 기존 모델과의 성능 비교:
  ○ ImageNet, VTAB benchmark에서 ViT기반 모델이 BiT(기존 SOTA)모델보다 성능 우수
  ○ 연산량이 동일하다면, ViT가 BiT보다 성능 우수

- 적용 범위:
  ○ Large dataset(JFT)에서 ViT가 BiT보다 우수
  ○ Small dataset(ImageNet)에서 BiT가 ViT보다 우수

## 5. 비판적 평가

### 5.1 강점
- 아키텍쳐적 혁신: CNN 모델 탈피
  ○ CV(Computer Vison) 분야에서 Transformer 인코더 구조를 순전히 활용(구조적 단순함)
  ○ 초기 layer에서 locality와 globality 모두 학습
  ○ 뒷 layer에서 globality 집중 학습
  ○ CV 분야에서 attention 기법을 활용한 연구 중 가장 높은 resolution을 다룸

- 성능 향상:
  ○ Image classification 분야 SOTA 달성
  ○ 기존 모델 대비 효율적 연산

- 이론적 기여:
  ○ 거거익선(거대한 모델과 거대한 데이터일수록 성능 우상향) 증명

### 5.2 한계점
- 방법론적 한계
  ○ Detection, Segmentation과 같은 다른 task에 대해 적용 불가(낮은 범용성)
  ○ Self-supervised learning 활용 시, 기존 모델보다 낮은 성능

- 실험적 한계
  ○ 작은 dataset에서 기존 모델이 더 좋은 성능을 보임
  ○ High resolution 활용 pre-training 통계 부족

### 5.3 개선 가능성
- 구조 개선:
  ○ 작은 Dataset에서 inductive bias 의도적 활용(MSA의 QK^T 연산 시 이웃하는 patch에 +1.2배, 중간 지점 +1.0배, 멀리 떨어진 지점 +0.8배) 
  ○ Hardware 구조에 맞춰, 메모리 공간의 연속적인 Axial Multi-Head Attention 활용

- Fine-Tuning 추가 실험:
  ○ NLP(Natural Language Processing)에서 활용한 방법과 같이 output layer를 추가하여, 더 많 task에 맞춰 fine-tuning 및 자세한 분

- 새로운 응용:
  ○ Image generator: Transformer decoder를 추가하여, 이미지 생성
  ○ Text + Image 병렬 encoding: 특정 형상(강아지, 차량 모양의 사진)과 텍스트('강아지', '자동차') mapping 학습
  ○ Image 병렬 embedding: 여러 이미지를 하나의 sequence로 입력(training set이 충분히 많을 때, pre-training 수행 시간 감소 목적)
  ○ 안면인식만 활용하여, 유전율 예측
  ○ 군사기지 분석: 군사기지 내부 여러 시설(무기고, 관제탑, 정비소)의 위성 사진 특징을 학습하여, 정확한 타겟 맵 만들기

## 6. 관련 연구와의 관계

### 6.1 선행 연구와의 연관성
- 이론적 배경
  ○ NLP의 Transformer encoder 구조 활용(Devlin et al. (2019))
  ○ 2x2 small resolution에서 Vision Transformer 활용(Cordonnier et al. (2020))
  ○ 픽셀 단위 self-attention(Parmar et al. (2018))

- 방법론적 연관성
  ○ Transformer의 encoder를 CV에서 Image classification을 목적으로 활용
  ○ medium-resolution에서 Vision Transformer 활용 성공
  ○ Patch 단위 self-attention 진행

- 동일 문제에 대한 다른 접근법
  ○ CNN 기반 모델(BiT) 활용 Image classification(Kolesnikov et al., 2020)
  ○ CNN과 self-attention을 결합한 hybrid 모델(Wang et al. (2018))

### 6.2 차별점
- 새로운 기술적 접근법
  ○ 2D image를 1D로 span하여 Transformer의 input으로 사용
  ○ Text가 아닌 Image 관점의 Transformer 사용
  ○ Transformer encoder의 원래 구조 반영과 CNN 구조 배제(inductive bias 극복하며 성능 대폭 향상)

## 7. AIoT 연구에의 적용 가능성

### 7.1 연구실 주제와의 연관성
- [우리 연구실의 AIoT 주제와의 연관성 개조식으로 작성]
- [관련 연구 방향]
  * [연구 방향 1과 설명]
  * [연구 방향 2와 설명]
- [기술적 활용 가능성]
- [이론적 확장 가능성]

### 7.2 잠재적 응용 분야
- [논문 기술의 AIoT 응용 분야 개조식으로 작성]
- [센서 데이터 분석 응용]
  * [응용 1과 설명]
  * [응용 2와 설명]
- [에지 컴퓨팅 응용]
- [스마트 시스템 응용]

### 7.3 구현/적용 계획
- [실제 구현 및 적용 계획 개조식으로 작성]
- [단계별 구현 방법]
  * [단계 1과 설명]
  * [단계 2와 설명]
- [필요한 자원 및 도구]
- [예상되는 결과 및 효과]

## 8. 참고 문헌
1. Vaswani, A., Shazeer, N., Parmar, N., Uszkoreit, J., Jones, L., Gomez, A. N., Kaiser, Ł., & Polosukhin, I. (2017). Attention is all you need. Advances in Neural Information Processing Systems (NeurIPS).
2. Devlin, J., Chang, M. W., Lee, K., & Toutanova, K. (2019). BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding. Proceedings of the NAACL-HLT.
3. He, K., Zhang, X., Ren, S., & Sun, J. (2016). Deep residual learning for image recognition. Proceedings of the IEEE conference on computer vision and pattern recognition (CVPR).
4. Kolesnikov, A., Beyer, L., Zhai, X., Puigcerver, J., Yung, J., Gelly, S., & Houlsby, N. (2020). Big Transfer (BiT): General Visual Representation Learning. European Conference on Computer Vision (ECCV).
5. Cordonnier, J.-B., Loukas, A., & Jaggi, M. (2020). On the Relationship between Self-Attention and Convolutional Layers. International Conference on Learning Representations (ICLR).
6. Parmar, N., Vaswani, A., Uszkoreit, J., Kaiser, L., Shazeer, N., Ku, A., & Tran, D. (2018). Image Transformer. International Conference on Machine Learning (ICML).
7. Sun, C., Shrivastava, A., Singh, S., & Gupta, A. (2017). Revisiting unreasonable effectiveness of data in deep learning era. Proceedings of the IEEE international conference on computer vision (ICCV).
8. Russakovsky, O., Deng, J., Su, H., Krause, J., Satheesh, S., Ma, S., ... & Fei-Fei, L. (2015). ImageNet Large Scale Visual Recognition Challenge. International Journal of Computer Vision (IJCV).

## 9. 용어 정리
1. ViT(Vision Transformer): 이미지를 패치 단위로 분할하여 Transformer 인코더에 입력하는 방식의 이미지 인식 모델
2. MSA(Multi-Head Self-Attention): 입력 시퀀스 내의 모든 요소(패치) 간의 관계를 여러 개의 헤드(Head)로 나누어 병렬적으로 계산하여 다양한 특징을 추출하는 메커니즘
3. MLP(Multi-Layer Perceptron): 입력층과 출력층 사이에 하나 이상의 은닉층이 존재하는 신경망으로, ViT에서는 인코더 내부의 피드포워드 네트워크(FFN)를 지칭함
4. BiT(Big Transfer): 대규모 데이터셋으로 사전 학습된 ResNet 기반의 CNN 모델로, 본 논문에서 ViT의 성능을 비교 검증하기 위한 베이스라인(Baseline)으로 사용됨
5. Inductive Bias: 모델이 학습하지 않은 데이터에 대해 예측을 수행할 때 사용하는 추가적인 가정. (예: CNN은 지역성(Locality)과 평행이동 불변성(Translation Equivariance)이라는 강한 편향을 가짐)
6. Patch Embedding: 2차원 이미지를 고정된 크기의 조각(Patch)으로 자르고, 이를 선형 변환(Linear Projection)하여 1차원 벡터 시퀀스로 만드는 과정
7. [class] token: BERT에서 차용한 개념으로, 입력 시퀀스의 맨 앞에 추가되는 학습 가능한 특수 벡터. 인코더를 통과한 후 이 토큰의 출력값이 이미지 전체의 표현(Global Representation)으로 간주되어 분류에 사용
8. Fine-tuning: 대규모 데이터셋으로 사전 학습(Pre-training)된 모델의 가중치를 초기값으로 사용하여, 해결하고자 하는 특정 작업(Downstream Task)의 데이터셋에 맞게 추가로 학습시키는 과정

## 10. 추가 참고 사항

- [논문 관련 코드 저장소]
- [추가 자료 및 리소스]
- [관련 토론 및 후속 연구]
- [구현 시 참고할 사항]

---

**리뷰어**: [이름]  
**리뷰 일자**: [YYYY-MM-DD]  
**토론 사항**: [논문 토론 시 주요 논의점을 메모]

# 개조식 논문 리뷰 작성 지침

1. **간결성 유지**:
   - 문장보다는 구(phrase) 중심으로 작성
   - 핵심 정보만 포함하고 불필요한 설명 제외
   - 중요한 내용은 굵은 글씨로 강조

2. **계층적 구조 활용**:
   - 글머리 기호(bullet points)로 정보 계층화
   - 들여쓰기를 통해 상위-하위 관계 표현
   - 관련 정보는 그룹화하여 제시

3. **약어 표기법**:
   - 모든 약어는 최초 등장 시 전체 단어와 간략한 정의 함께 제공
   - 예: CNN(Convolutional Neural Network: 합성곱 신경망)
   - 이후 등장 시에는 약어만 사용 가능

4. **객관성 유지**:
   - 사실과 논문의 주장을 명확히 구분
   - 개인적 의견은 '비판적 평가' 섹션에 한정
   - 논문의 내용을 왜곡하지 않도록 주의

5. **참고 자료 활용**:
   - 논문 이해에 도움이 되는 추가 자료 참조
   - 관련 연구와의 연결점 탐색
   - 코드 구현이 있는 경우 코드 분석 결과 포함

---

**작성자**: 임재현 
**작성일**: 2025-02-03  
**토론 사항**: 
- [내용1]
- [내용2]
