# 논문 리뷰 템플릿 - 개조식 작성 가이드

> 이 템플릿은 개조식으로 논문을 효과적으로 리뷰하기 위한 구조를 제공하며, 필요에 따라 구조와 내용을 삭제, 수정 및 추가하여 사용


## 1. 논문 기본 정보

- **제목**: AN IMAGE IS WORTH 16X16 WORDS: TRANSFORMERS FOR IMAGE RECOGNITION AT SCALE
- **저자**: Alexey Dosovitskiy, Lucas Beyer, Alexander Kolesnikov, Dirk Weissenborn, Xiaohua Zhai, Thomas Unterthiner, Mostafa Dehghani, Matthias Minderer, Georg Heigold, Sylvain Gelly, Jakob Uszkoreit, Neil Houlsby
- **학회/저널**: International Conference on Learning Representations(ICLR)
- **년도**: 2021
- **DOI/URL**: https://arxiv.org/pdf/2010.11929
- **키워드**: Computer Vison, Vision Transformer(ViT), Inductive bias, Muti-Head Attention, Patch Embedding

## 2. 논문 요약

### 2.1 연구 목적 및 문제 정의
- CNN을 사용하지 않는 새로운 이미지 인식 모델 제안

- 기존 CNN 기반 모델의 문제점:
  ○ 현대 가속기의 효율적 연산 적용 어려움
  ○ Inductive bias로 인한 대량 데이터에서 성능 한계

- 트랜스포머 구조(Multi-Head attention 기반 인코더 활용)를 활용한 이미지 인식 모델 제안


### 2.2 주요 접근 방법
- 트랜스포머 구조를 활용한 이미지 인식 아키텍쳐 제안

- 주요 구성 요소
  ○ Patch+Position Embedding
  ○ 트랜스포머 인코더
  ○ Multi-Layer Perceptron(MLP: 선형변환과 비선형함수로 이루어진 다층 구조)

### 2.3 주요 결과
- ImageNet Top1 accuracy 점수 87.76% 달성(ViT-L/16)
  ○ BiT-L 모델보다 0.22% 향상

- VTAB에서 점수 77.63% 달성(ViT-H/14)
  ○ BiT-L 모델보다 1.34% 향상

- 주요 발견점
  ○ Inductive bias 극복
  ○ Scaling으로 지속적 성능 향상 가능
  ○ 1D Positional Embedding과 2D Positional Embedding 사이 유의미한 차이 없음

## 3. 방법론 분석

### 3.1 제안 방법 상세 설명
- Vision Transformer 구성
  ○ Embedding
    ■ Patch Embedding: Patch를 N개의 1D sequence로 자른 이후, Linear projection을 통해 D dimension 변환
    ■ Position Embedding: NxD Embedding matrix E_pos를 Patch embedding 값에 더해줌
  ○ 인코더
    ■ 각 서브층에 Layer Normalization과 Residual Connection 활용
    ■ 하나의 Multi-Head Attention layer와 Multi-Layer Perceptron(MLP)로 구성
  ○ MLP Head
    ■ 추가적인 class token의 인코더 output을 MLP Head의 input으로 삽입
    ■ 활성화 함수가 tanh인 하나의 layer로 구성됨
    ■ K dimension으로 변환하여 최종 class 예측

### 3.2 핵심 알고리즘/모델
- Embedding representation:
  ○ 수식: z_0 = [x_class; (x_p)^1*E; (x_p)^2*E;...; (x_p)^N*E]+E_pos
    ■ z_0: Transformer 인코더에 들어가는 초기 input sequence
    ■ x_class: 학습 가능한 [class] token
    ■ x_p^i: i번째 image patch
    ■ E: D dimension 변환 행렬(Linear Projection).
    ■ E_pos: 위치 정보를 담고 있는 embedding vector
  ○ 2D input patch를 1D로 펼쳐서 넣음
  ○ position embedding vector를 추가하여 order에 대한 정보 학습
  ○ 모델이 학습 가능한 D dimension으로 변환

- Multi-Head Attention + Residual Connection:
  ○ 수식: z'_l = MSA(LN(z_(l−1))) + z_(l−1)
    ■ z'_l: l번째 Multi-Head Attention sub-layer의 output 
    ■ z_(l-1): l-1번째 layer의 output
    ■ LN: Layer Normalization
    ■ MSA: Multi-Head Attention 연산(Attention is All you need 리뷰)
  ○ 다양한 관점에서 정보 종합
  ○ 기울기 소실 문제 완화(deep layer 형성 가능)

- Multi-Layer Perceptron(MLP):
  ○ 수식: z_l = MLP(LN(z'_l)) + z'_l
    ■ z_l: l번째 layer의 output
    ■ MLP: 두 개의 linear projection과 하나의 gelu activation function으로 구성

### 3.3 실험 설계
- 데이터셋(pre-training):
  ○ ImageNet (1K classes, 1.3M images)
  ○ ImageNet-21k (21k classes, 14M images)
  ○ JFT (18k classes, 303M images)

- 모델 구성:
  ○ layer 수: 12
  ○ hidden size D dimension(MSA 내부): 768
  ○ MLP dimension: 3072
  ○ head 수: 12
  ○ Parameter 수: 86M

- 최적화 방법:
  ○ Adam optimizer 사용
  ○ Warmup 후 cosine decay 사용
  ○ Dropout 사용
  ○ Label smoothing 사용

## 4. 주요 결과 분석

### 4.1 정량적 결과
- 이미지 인식
  ○ ImageNet: 87.76%(ViT-L/16)(BiT-L 대비 +0.022%)
  ○ VTAB(19 tasks): 77.63%(ViT-H/14)(BiT-L 대비 +1.34%)

- 계산 효율성:
  ○ 계산 크기가 동일하다면, ViT가 BiT보다 정확

- 통계적 유의성
  ○ 만일 Pre-training dataset이 작다면, BiT가 더 좋은 성능을 가질 수 있음  

### 4.2 정성적 결과]
- CV 분야에서 attension 기법을 활용하여 medium-level의 resolution을 다룸
- Inductive bias 극복(saturation되지 않음, 초기 layer에서도 global information을 가짐)
- Self-supervised ViT의 가능성 보임
- 1D와 2D positional embedding간 차이가 크지 않음(계산 효율성을 위해 1D 사용)
- Attention rollout으로 sementically relevant regions을 정확하게 학습함을 증명


### 4.3 비교 분석
- [제안 방법과 기존 방법 간 비교 개조식으로 작성]
- [성능 측면의 비교]
  * [장점 1과 설명]
  * [장점 2와 설명]
- [효율성 측면의 비교]
- [적용 범위 및 한계의 비교]
- 기존 모델과의 성능 비교:
  ○ ImageNet, VTAB benchmark에서 ViT기반 모델이 BiT(기존 SOTA)모델보다 성능 우수
  ○ 연산량이 동일하다면, ViT가 BiT보다 성능 우수

- 적용 범위:
  ○ Large dataset(JFT)에서 ViT가 BiT보다 우수
  ○ Small dataset(ImageNet)에서 BiT가 ViT보다 우수

이미지 인식
  ○ ImageNet: 87.76%(ViT-L/16)(BiT-L 대비 +0.022%)
  ○ VTAB(19 tasks): 77.63%(ViT-H/14)(BiT-L 대비 +1.34%)

- 계산 효율성:
  ○ 계산 크기가 동일하다면, ViT가 BiT보다 정확


## 5. 비판적 평가

### 5.1 강점
- [논문의 주요 강점 개조식으로 작성]
- [기술적 혁신 측면]
  * [혁신 1과 설명]
  * [혁신 2와 설명]
- [성능 향상 측면]
- [이론적 기여 측면]
- CV(Computer Vison) 분야에서 Transformer 인코더 구조를 순전히 활용
- CV 분야에서 attension 기법을 활용한 연구 중 가장 높은 resolution을 다룸
-

### 5.2 한계점
- [논문의 주요 한계점 개조식으로 작성]
- [방법론적 한계]
  * [한계 1과 설명]
  * [한계 2와 설명]
- [실험적 한계]
- [가정 및 제약사항]
- Dataset 크기 제한
  ○ 작은 dataset에서 기존 모델이 더 좋은 성능을 보임

### 5.3 개선 가능성
- [논문의 개선 가능성 개조식으로 작성]
- [방법론 개선 방향]
  * [개선 방향 1과 설명]
  * [개선 방향 2와 설명]
- [추가 실험 제안]
- [새로운 응용 분야 제안]
- 구조 개선:
  ○ 작은 Dataset에서 indutive bias 의도적 활용(MSA의 QK^T 연산 시 이웃하는 patch에 +1.2배, 중간 지점 +1.0배, 멀리 떨어진 지점 +0.8배) 
  ○ Hardware 구조에 맞춰, 메모리 공간의 연속적인 Axial Multi-Head Attention 활용
- Fine-Tuning 추가 실험:
  ○ NLP(Natural Language Processing)에서 활용한 방법과 같이 output layer를 추가하여, 특정 task에 맞춰 fine-tuning
- 새로운 응용:
  ○ Image generator: Transformer decoder를 추가하여, 이미지 생성
  ○ Text + Image 병렬 encoding: 특정 형상(강아지, 차량 모양의 사진)과 텍스트('강아지', '자동차') mapping 학습
  ○ Image 병렬 embedding: 여러 이미지를 하나의 sequence로 입력(training set이 충분히 많을 때, pre-training 수행 시간 감소 목적)
  ○ 안면인식만 활용하여, 유전율 예측
  ○ 군사기지 분석: 군사기지 내부 여러 시설(무기고, 관제탑, 정비소)의 위성 사진 특징을 학습하여, 정확한 타겟 맵 만들기

## 6. 관련 연구와의 관계

### 6.1 선행 연구와의 연관성
- [기반이 되는 선행 연구 개조식으로 작성]
- [이론적 배경 및 영향]
  * [영향 1과 설명]
  * [영향 2와 설명]
- [방법론적 연관성]
- [동일 문제에 대한 다른 접근법들]
- BERT
- Parmar
- weissenborn
- cordonnier
--->이건 너무 많다 적어달라하자


### 6.2 차별점
- [선행 연구와의 차별점 개조식으로 작성]
- [새로운 기술적 접근법]
  * [차별점 1과 설명]
  * [차별점 2와 설명]
- [성능 개선 측면의 차별점]
- [문제 정의 및 해결 방식의 차이점]

## 7. AIoT 연구에의 적용 가능성

### 7.1 연구실 주제와의 연관성
- [우리 연구실의 AIoT 주제와의 연관성 개조식으로 작성]
- [관련 연구 방향]
  * [연구 방향 1과 설명]
  * [연구 방향 2와 설명]
- [기술적 활용 가능성]
- [이론적 확장 가능성]

### 7.2 잠재적 응용 분야
- [논문 기술의 AIoT 응용 분야 개조식으로 작성]
- [센서 데이터 분석 응용]
  * [응용 1과 설명]
  * [응용 2와 설명]
- [에지 컴퓨팅 응용]
- [스마트 시스템 응용]

### 7.3 구현/적용 계획
- [실제 구현 및 적용 계획 개조식으로 작성]
- [단계별 구현 방법]
  * [단계 1과 설명]
  * [단계 2와 설명]
- [필요한 자원 및 도구]
- [예상되는 결과 및 효과]

## 8. 참고 문헌

- [논문 리뷰 작성 중 추가로 참고한 문헌들 나열]
- [형식: 저자. (연도). 제목. 출처.]

## 9. 용어 정리

- **[약어 1]** ([원래 단어]: [간략한 정의])
- **[약어 2]** ([원래 단어]: [간략한 정의])
- **[용어 1]**: [정의 및 설명]
- **[용어 2]**: [정의 및 설명]

## 10. 추가 참고 사항

- [논문 관련 코드 저장소]
- [추가 자료 및 리소스]
- [관련 토론 및 후속 연구]
- [구현 시 참고할 사항]

---

**리뷰어**: [이름]  
**리뷰 일자**: [YYYY-MM-DD]  
**토론 사항**: [논문 토론 시 주요 논의점을 메모]

# 개조식 논문 리뷰 작성 지침

1. **간결성 유지**:
   - 문장보다는 구(phrase) 중심으로 작성
   - 핵심 정보만 포함하고 불필요한 설명 제외
   - 중요한 내용은 굵은 글씨로 강조

2. **계층적 구조 활용**:
   - 글머리 기호(bullet points)로 정보 계층화
   - 들여쓰기를 통해 상위-하위 관계 표현
   - 관련 정보는 그룹화하여 제시

3. **약어 표기법**:
   - 모든 약어는 최초 등장 시 전체 단어와 간략한 정의 함께 제공
   - 예: CNN(Convolutional Neural Network: 합성곱 신경망)
   - 이후 등장 시에는 약어만 사용 가능

4. **객관성 유지**:
   - 사실과 논문의 주장을 명확히 구분
   - 개인적 의견은 '비판적 평가' 섹션에 한정
   - 논문의 내용을 왜곡하지 않도록 주의

5. **참고 자료 활용**:
   - 논문 이해에 도움이 되는 추가 자료 참조
   - 관련 연구와의 연결점 탐색
   - 코드 구현이 있는 경우 코드 분석 결과 포함

---

**작성자**: [이름]  
**작성일**: YYYY-MM-DD  
**토론 사항**: 
- [내용1]
- [내용2]
