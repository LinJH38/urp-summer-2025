# 논문 리뷰 템플릿 - 개조식 작성 가이드

> 이 템플릿은 개조식으로 논문을 효과적으로 리뷰하기 위한 구조를 제공하며, 필요에 따라 구조와 내용을 삭제, 수정 및 추가하여 사용


## 1. 논문 기본 정보

- **제목**: Attention Is All You Need
- **저자**: Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N. Gomez, Łukasz Kaiser, Illia Polosukhin
- **학회/저널**: Neural Information Processing Systems (NeurIPS)
- **년도**: 2017
- **DOI/URL**: https://arxiv.org/abs/1706.03762
- **키워드**: Transformer,Multi-Head Attention, Scaled Dot-Product Attention, Positional Encoding, Adam optimizer

## 2. 논문 요약

### 2.1 연구 목적 및 문제 정의
- Self-attention만 사용하여, Transduction model 생성
- 기존 모델의 문제:
  ○ Distance Dependeny문제: 기존 모델은 거리가 증가함에 따라, 연산량이 선형 혹은 로그함수에 비례하여 증가
  ○ Paralleization 문제: Recurrent models은 이전 states에 연산이 끝나야, 현재 sequnce t에 대한 연산 가능
- 연구의 필요성
  ○ Sequence 길이와 무관하게 정확한 예측 필요
  ○ Paralleization 연산으로 시간 단축
중요성, 필요성
### 2.2 주요 접근 방법
- Multi-Head attention을 활용한 Paralleization 연산 구조의 Architecture 제안 
- 주요 구성 요소:
  ○ Encoder: Multi-Head Attention layer와 FF layer로 구성, Output을 Decoder의 중간 layer의 input으로 삽입
  ○ Decoder: Masked Multi-Head Attention layer와 기존 Multi-Head attention layer, FF layer로 구성
  ○ Multi-Head Attention: 다른 위치의 다른 표현 공간에서 다양한 정보를 Attention
  ○ Position-wise Feed-Forward Networks: 비선형성으로 복잡한 추론 가능
  ○ Positional Encoding: sin함수를 이용하여 input의 order를 나타냄
  ○ Embedding: 문자(input)을 숫자(index)로 변환
  ○ Softmax: 다음 올 단어(output)을 예측

### 2.3 주요 결과
- WMT 2014 English-to-German translation task에서 BLEU 최고점수(EN-DE:28.4, EN-FR:41.8)달성
- EN-DE에서 기존 방법(ConvS2S Ensemble)보다 2.04 더 높은 점수, EN-FR에서 0.49 더 높은 점수
- 성능은 높힌채로, Training Cost는 0.25배 감소
- Translation task가 아닌 Constituency Parsing에서 기존 방법보다 좋은 성능 보임

## 3. 방법론 분석

### 3.1 제안 방법 상세 설명
-Transformer 아키텍쳐 구성:
  ○ Embedding, Positional Encoding: 1개의 층으로 구성
    ■ 문자로 들어 온 Source sequence를 Preprocessing을 통해 구현 한 단어 사전(vector)에 맞춰 index 부여
    ■ input dimension을 512로 변경
    ■ Order 정보를 기억하기 위해, sin 함수를 이용하여 [-1,1] 사이의 서로 다른 값을 더함
  ○ Encoder: 6개의 동일한 층으로 구성
    ■ Multi-Head Attention layer를 통해 기존 input의 고차원적인 결합 형태 가져옴
    ■ 각 layer마다 residual(backpropagation에서 vanising gradient를 막기 위함)과 layer normalization(training과정에서
      previous layer의 weight가 달라지면 해당 layer output의 mean과 variance도 달라지므로 next layer에서 iteration마다 의도와 다르게 학습함
      따라서 layer normalization을 통해 layer의 input의 distribution을 비교적 동일하게 유지함으로써 학습 안정성을 높힘) 활용
    ■ Feed Forward layer를 통해 vector의 각 element 사이 의미 학습
    ■ Output을 동일한 층 decoder내부 Multi-Head Attention의 key와 value에 대한 input vector로 삽입  
  ○ Decoder: 6개의 동일한 층으로 구성
    ■ 첫번째 Multi-Head Attension layer에서 Mask 기법 사용(cheating 방지)
    ■ Encoder의 sub-layer 구조와 Multi-Head Attension layer 추가(총 3개의 layer로 구성)
  ○ Linear transformation, Soft max: 1개의 층으로 구성
    ■ 마지막 층에서 나온 decoder의 output(512 dimension vector)을 단어 사전 크기에 알맞은 dimension vector로 변환
    ■ 변환된 Vector의 각 elements에 Softmax function을 사용하여 단어 사전의 각 token에 대한 probabillity로 변환

### 3.2 핵심 알고리즘/모델
- Scaled Dot-Product Attention
  ○ 수식: Attention(Q, K, V ) = softmax(QK^T/√d_k)V
    ■ Q(쿼리): 찾고자 하는 정보
    ■ K(키): 참조할 정보의 색인
    ■ V(값): 실제 추출할 정보
    ■ d_k: 키 벡터의 차원
  ○ 기존 self-attention 식에 √d_k를 나눠 key vector의 차원 증가에 따른 vanising gradient(x값이 클 때, softmax(x)의 기울기)현상을 방지함 

- Multi-Head Attention
  ○ 수식: MultiHead(Q, K, V ) = Concat(head_1, ..., head_h)W^O
    ■ head_i = Attention(QW_i^Q, KW_i^K, VW_i^V)
    ■ W(weight matirx): 각 변수에 사용되는 parameters matirx
  ○ Multi-Head 방식으로 각 head가 주목하는 정보가 서로 다름
  ○ 각 head를 concatinate하여 하나의 vector로 변환
  ○ Linear projection을 통해 각 head가 지닌 정보를 조합하여 512 dimension으로 변환(축소)

- Position-wise Feed-Forward Networks
  ○ 수식: FFN(x) = max(0, xW_1 + b_1)W_2 + b_2
  ○ Non-linearity 성질을 추가하여, 복잡한 구조를 학습할 수 있도록
  ○ 각 위치별로 독립적이고 동일하게 적용

- Positional Encoding
  ○ 수식:
    ■ PE_(pos,2i) = sin(pos/10000^(2i/d_model))
    ■ PE_(pos,2i+1) = cos(pos/10000^(2i/d_model))
  ○ sin 함수를 이용하여 input의 order를 나타냄(새로운 방식)

### 3.3 실험 설계
- 데이터셋
  ○ WMT 2014 English-German dataset (4.5M sentence pairs)
  ○ WMT 2014 English-French dataset (36M sentence pairs)

- 모델 구성
  ○ 층수: 6
  ○ Modle dimension: 512
  ○ Fead Foward layer dimension: 2048
  ○ head수: 8

- 최적화 방법
  ○ Adam optimizer 사용: momentum과 Adaptive learning rate 활용
  ○ Warmup step 사용: linear하게 learning rate를 증가시키고, warmup step(4000)이후 decay
  ○ Residual connection 사용
  ○ Drop out 사용: P_drop = 0.1, overfitting 방지 및 ensemble 효과
  ○ Label smoothing 사용: Overfitiing 방지

## 4. 주요 결과 분석

### 4.1 정량적 결과
- Machine Translation
  ○ WMT 2014 English-to-German translation: BLEU(28.4)
  ○ WMT 2014 English-to-French0 translation: BLEU(41.8)
  ○ 기존 모델보다 각 2.04, 0.49 더 높은 점수

- Training Cost
  ○ 기존 모델보다 0.25배 감소

### 4.2 정성적 결과

- Attention Visualizations
  ○ 하나의 단어와 각 head가 주목하는 단어 사이 관계 표현

- 장거리 의존성 처리
  ○ 문장 내 멀리 떨어진 요소 간 관계 파악 능력 향상

- Single-Head 문제 해결
  ○ Multi-Head 방식을 통해 특정 관점에 특화된 head 정보 획득
  ○ 각 head를 조합하여 더 정확하게 예측

### 4.3 비교 분석
- 장거리 의존성 처리
  ○ 문장 내 멀리 떨어진 요소 간 관계 파악 능력 향상
- Single-Head 문제 해결
  ○ Multi-Head 방식을 통해 특정 관점에 특화된 head 정보 획득
  ○ 각 head를 조합하여 더 정확하게 예측

## 5. 비판적 평가

### 5.1 강점
- Transformer 구조 사용
  ○ Paralleization 연산
- Computaion complexity 감소
- Translation 정확도 향상
- 다른 sequence to sequence task 확장 가능

### 5.2 한계점 [아직 안 보임]
- [논문의 주요 한계점 개조식으로 작성]
- [방법론적 한계]
  * [한계 1과 설명]
  * [한계 2와 설명]
- [실험적 한계]
- [가정 및 제약사항]
- 계산 복잡성
  ○ Sequence length의 제곱에 비례

### 5.3 개선 가능성
- [논문의 개선 가능성 개조식으로 작성]
- [방법론 개선 방향]
  * [개선 방향 1과 설명]
  * [개선 방향 2와 설명]
- [추가 실험 제안]
- [새로운 응용 분야 제안]

## 6. 관련 연구와의 관계

### 6.1 선행 연구와의 연관성
- [기반이 되는 선행 연구 개조식으로 작성]
- [이론적 배경 및 영향]
  * [영향 1과 설명]
  * [영향 2와 설명]
- [방법론적 연관성]
- [동일 문제에 대한 다른 접근법들]
- Self-Attention 기법 활용
  ○ Bahdanau et al.(2014)의 어텐션 개념 확장 및 개선
- 방법론적 연관성
  ○ Layer Normalization
  ○ Dropout
  ○ Residual Connection
  ○ Adam Optimization
  ○ Byte Pair Encoding

### 6.2 차별점
- [선행 연구와의 차별점 개조식으로 작성]
- [새로운 기술적 접근법]
  * [차별점 1과 설명]
  * [차별점 2와 설명]
- [성능 개선 측면의 차별점]
- [문제 정의 및 해결 방식의 차이점]

## 7. AIoT 연구에의 적용 가능성

### 7.1 연구실 주제와의 연관성
- [우리 연구실의 AIoT 주제와의 연관성 개조식으로 작성]
- [관련 연구 방향]
  * [연구 방향 1과 설명]
  * [연구 방향 2와 설명]
- [기술적 활용 가능성]
- [이론적 확장 가능성]

### 7.2 잠재적 응용 분야
- [논문 기술의 AIoT 응용 분야 개조식으로 작성]
- [센서 데이터 분석 응용]
  * [응용 1과 설명]
  * [응용 2와 설명]
- [에지 컴퓨팅 응용]
- [스마트 시스템 응용]

### 7.3 구현/적용 계획
- [실제 구현 및 적용 계획 개조식으로 작성]
- [단계별 구현 방법]
  * [단계 1과 설명]
  * [단계 2와 설명]
- [필요한 자원 및 도구]
- [예상되는 결과 및 효과]

## 8. 참고 문헌

- [논문 리뷰 작성 중 추가로 참고한 문헌들 나열]
- [형식: 저자. (연도). 제목. 출처.]

## 9. 용어 정리

- **[약어 1]** ([원래 단어]: [간략한 정의])
- **[약어 2]** ([원래 단어]: [간략한 정의])
- **[용어 1]**: [정의 및 설명]
- **[용어 2]**: [정의 및 설명]

## 10. 추가 참고 사항

- [논문 관련 코드 저장소]
- [추가 자료 및 리소스]
- [관련 토론 및 후속 연구]
- [구현 시 참고할 사항]

---

**리뷰어**: [이름]  
**리뷰 일자**: [YYYY-MM-DD]  
**토론 사항**: [논문 토론 시 주요 논의점을 메모]

# 개조식 논문 리뷰 작성 지침

1. **간결성 유지**:
   - 문장보다는 구(phrase) 중심으로 작성
   - 핵심 정보만 포함하고 불필요한 설명 제외
   - 중요한 내용은 굵은 글씨로 강조

2. **계층적 구조 활용**:
   - 글머리 기호(bullet points)로 정보 계층화
   - 들여쓰기를 통해 상위-하위 관계 표현
   - 관련 정보는 그룹화하여 제시

3. **약어 표기법**:
   - 모든 약어는 최초 등장 시 전체 단어와 간략한 정의 함께 제공
   - 예: CNN(Convolutional Neural Network: 합성곱 신경망)
   - 이후 등장 시에는 약어만 사용 가능

4. **객관성 유지**:
   - 사실과 논문의 주장을 명확히 구분
   - 개인적 의견은 '비판적 평가' 섹션에 한정
   - 논문의 내용을 왜곡하지 않도록 주의

5. **참고 자료 활용**:
   - 논문 이해에 도움이 되는 추가 자료 참조
   - 관련 연구와의 연결점 탐색
   - 코드 구현이 있는 경우 코드 분석 결과 포함

---

**작성자**: 임재현
**작성일**: 2025-01-30  
**토론 사항**: 
- [내용1]
- [내용2]
